"""ä½¿ç”¨æ”¶é›†åˆ°æ•°æ®è¿›è¡Œè®­ç»ƒ"""
import os
import random
from collections import defaultdict, deque

import numpy as np
import pickle
import time

import zip_array
from config import CONFIG
from game import Game, Board
from mcts import MCTSPlayer
from mcts_pure import MCTS_Pure

if CONFIG['use_redis']:
    import my_redis, redis
    import zip_array

if CONFIG['use_frame'] == 'paddle':
    from paddle_net import PolicyValueNet
elif CONFIG['use_frame'] == 'pytorch':
    from pytorch_net import PolicyValueNet
else:
    print('æš‚ä¸æ”¯æŒæ‚¨é€‰æ‹©çš„æ¡†æ¶')

# æ ¹æ“š no_dark_mode è¨­å®šä¸åŒæª”æ¡ˆä½ç½®
if CONFIG.get('no_dark_mode', True):
    TRAIN_DATA_PATH = CONFIG.get('train_data_buffer_path_no_dark', 'train_data_buffer_no_dark.pth')
    BEST_POLICY_PATH = 'best_policy_no_dark.pth'
    CURRENT_POLICY_PATH = 'current_policy_no_dark.pth'
    MODELS_DIR = 'models_no_dark'
    REDIS_KEY = 'train_data_buffer_no_dark'
    REDIS_ITERS_KEY = 'iters_no_dark'
else:
    TRAIN_DATA_PATH = CONFIG.get('train_data_buffer_path', 'train_data_buffer.pth')
    BEST_POLICY_PATH = 'best_policy.pth'
    CURRENT_POLICY_PATH = 'current_policy.pth'
    MODELS_DIR = 'models'
    REDIS_KEY = 'train_data_buffer'
    REDIS_ITERS_KEY = 'iters'

os.makedirs(MODELS_DIR, exist_ok=True)

class TrainPipeline:
    def __init__(self, init_model=None):
        # è‹¥ç£ç¢Ÿå·²æœ‰ best modelï¼Œå˜—è©¦è¼‰å…¥ï¼ˆå¯èƒ½ç‚º Noneï¼‰
        self.best_policy_net = None
        if os.path.exists(BEST_POLICY_PATH):
            try:
                self.best_policy_net = PolicyValueNet(model_file=BEST_POLICY_PATH)
                print(f"è¼‰å…¥ best policy: {BEST_POLICY_PATH}")
            except Exception as e:
                print(f"è¼‰å…¥ best policy å¤±æ•—: {e}")

        self.board = Board()
        self.game = Game(self.board)
        self.n_playout = CONFIG['play_out']
        self.c_puct = CONFIG['c_puct']
        self.learn_rate = 1e-3
        self.lr_multiplier = 1
        self.temp = 1.0
        self.batch_size = CONFIG['batch_size']
        self.epochs = CONFIG['epochs']
        self.kl_targ = CONFIG['kl_targ']
        self.check_freq = 100
        self.game_batch_num = CONFIG['game_batch_num']
        self.best_win_ratio = 0.0
        self.pure_mcts_playout_num = 50  # ç”¨æ–¼ pure MCTS baseline
        if CONFIG['use_redis']:
            self.redis_cli = my_redis.get_redis_cli()
        self.buffer_size = CONFIG['buffer_size']
        self.data_buffer = deque(maxlen=self.buffer_size)

        if init_model:
            try:
                self.policy_value_net = PolicyValueNet(model_file=init_model)
                print('å·²åŠ è½½ä¸Šæ¬¡æœ€ç»ˆæ¨¡å‹')
            except Exception as e:
                print('æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨æˆ–è½½å…¥å¤±è´¥ï¼Œä»é›¶å¼€å§‹è®­ç»ƒ', e)
                self.policy_value_net = PolicyValueNet()
        else:
            print('ä»é›¶å¼€å§‹è®­ç»ƒ')
            self.policy_value_net = PolicyValueNet()

    def _fight(self, player1, player2, n_games=30):
        """é›™æ–¹å°æˆ°ï¼Œè¿”å›å‹ç‡ã€è©³ç´°å‹è² """
        win_cnt = defaultdict(int)
        show = 0
        for i in range(n_games):
            if n_games == 10:
                show = 1
            if i % 2 == 0:
                winner = self.game.start_play(player1, player2, start_player=1, is_shown=show)
                if winner == 1:
                    win_cnt["p1"] += 1
                elif winner == 2:
                    win_cnt["p2"] += 1
                else:
                    win_cnt["tie"] += 1
            else:
                winner = self.game.start_play(player2, player1, start_player=1, is_shown=show)
                if winner == 1:
                    win_cnt["p2"] += 1
                    print()
                elif winner == 2:
                    win_cnt["p1"] += 1
                else:
                    win_cnt["tie"] += 1
            print("ç¬¬",i+1,"å ´çµæŸ",sep="")
        win_ratio = (win_cnt["p1"] + 0.5 * win_cnt["tie"]) / n_games
        return win_ratio, win_cnt

    def multi_evaluate(self, n_games=30):
        """åŒæ™‚æ¸¬è©¦ current vs best, current vs pureMCTS, best vs pureMCTS"""
        # å°å‡ºæª”æ¡ˆæ™‚é–“
        for path, name in [(CURRENT_POLICY_PATH, "current_policy"), (BEST_POLICY_PATH, "best_policy")]:
            if os.path.exists(path):
                print(f"[æª”æ¡ˆæ™‚é–“] {name} æœ€å¾Œä¿®æ”¹: {time.ctime(os.path.getmtime(path))}")

        current_player = MCTSPlayer(self.policy_value_net.policy_value_fn,
                                    c_puct=self.c_puct,
                                    n_playout=100)

        # baseline: pure MCTS
        pure_player = MCTS_Pure(self.pure_mcts_playout_num)

        # best_policy
        best_player = None
        if os.path.exists(BEST_POLICY_PATH):
            try:
                best_net = PolicyValueNet(model_file=BEST_POLICY_PATH)
                best_player = MCTSPlayer(best_net.policy_value_fn,
                                         c_puct=self.c_puct,
                                         n_playout=100)
            except Exception as e:
                print("[EVAL] è¼‰å…¥ best_policy å¤±æ•—:", e)

        results = {}

        # 1. current vs best
        if best_player:
            print("å­˜åœ¨best_policy é–‹å§‹evaluate",BEST_POLICY_PATH)
            r, cnt = self._fight(current_player, best_player, n_games=n_games)
            results["current_vs_best"] = (r, cnt)
            print(f"[EVAL] current vs best â†’ {cnt}, å‹ç‡={r:.3f}")
            results["current_vs_pure"] = (0, 0)
        else:
            "ç›®å‰ä¸å­˜åœ¨best_policy"
            r, cnt = self._fight(current_player, pure_player, n_games=10)
            results["current_vs_pure"] = (r, cnt)
            print(f"[EVAL] current vs pureMCTS â†’ {cnt}, å‹ç‡={r:.3f}")
        # 2. current vs pureMCTS


        return results

    def policy_evaluate(self, n_games=30):
        """
        Evaluate current policy against best policy if exists,
        otherwise against a pure MCTS baseline.
        è¿”å›èƒœç‡ (float)
        """
        # é‡æ–°å˜—è©¦è¼‰å…¥ disk ä¸Šçš„ best modelï¼ˆè‹¥è¢«å¤–éƒ¨ç¨‹åºæ›´æ–°ï¼‰
        if os.path.exists(BEST_POLICY_PATH):
            try:
                self.best_policy_net = PolicyValueNet(model_file=BEST_POLICY_PATH)
                print(f"[EVAL] ä½¿ç”¨å·²å­˜åœ¨çš„ best model: {BEST_POLICY_PATH}")
            except Exception as e:
                print(f"[EVAL] è¼‰å…¥ best model å¤±æ•—ï¼Œæ”¹ç”¨ pure MCTS baseline: {e}")
                self.best_policy_net = None
        else:
            print("[EVAL] ç„¡ best model æª”æ¡ˆï¼Œæ”¹ç”¨ pure MCTS baseline")

        # æ§‹é€ å°æ‰‹
        current_mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn,
                                         c_puct=self.c_puct,
                                         n_playout=max(50, self.n_playout // 4))  # è©•ä¼°å¯ä»¥ç”¨è¼ƒå°‘ playout
        if self.best_policy_net is not None:
            best_mcts_player = MCTSPlayer(self.best_policy_net.policy_value_fn,
                                          c_puct=self.c_puct,
                                          n_playout=max(50, self.n_playout // 4))
            baseline_is_pure_mcts = False
            print("[EVAL] baseline = best_policy_net")
        else:
            # ç”¨ pure MCTS ä½œ baselineï¼ˆä¸ä½¿ç”¨ç¥ç¶“ç¶²è·¯ï¼‰
            best_mcts_player = MCTS_Pure(self.pure_mcts_playout_num)
            baseline_is_pure_mcts = True
            print(f"[EVAL] baseline = pure MCTS ({self.pure_mcts_playout_num} playouts)")

        win_cnt = defaultdict(int)
        for i in range(n_games):
            # äº¤æ›å…ˆæ‰‹/å¾Œæ‰‹
            if i % 2 == 0:
                winner = self.game.start_play(current_mcts_player, best_mcts_player, start_player=1, is_shown=0)
                if winner == 1:
                    win_cnt["current"] += 1
                elif winner == 2:
                    win_cnt["best"] += 1
                else:
                    win_cnt["tie"] += 1
            else:
                winner = self.game.start_play(best_mcts_player, current_mcts_player, start_player=1, is_shown=0)
                # æ³¨æ„ï¼šç•¶ baseline ç‚º pure MCTSï¼Œbest_mcts_player ä¸¦é "best" ä½†è¨ˆæ•¸æ–¹å¼ä¸€è‡´
                if winner == 1:
                    # winner 1 å°æ–¼å‚³å…¥çš„ (best, current) ä»£è¡¨ best è´
                    win_cnt["best"] += 1
                elif winner == 2:
                    win_cnt["current"] += 1
                else:
                    win_cnt["tie"] += 1

        win_ratio = (win_cnt["current"] + 0.5 * win_cnt["tie"]) / n_games
        print(f"[æ¨¡å‹å°æˆ°] new vs baseline â†’ win: {win_cnt['current']}, lose: {win_cnt['best']}, tie: {win_cnt['tie']}")
        print(f"[EVAL] å‹ç‡ï¼ˆcurrent vs baselineï¼‰: {win_ratio:.3f}")
        return win_ratio

    def policy_updata(self):
        mini_batch = random.sample(self.data_buffer, self.batch_size)
        mini_batch = [zip_array.recovery_state_mcts_prob(data) for data in mini_batch]
        state_batch = np.array([data[0] for data in mini_batch], dtype='float32')
        mcts_probs_batch = np.array([data[1] for data in mini_batch], dtype='float32')
        winner_batch = np.array([data[2] for data in mini_batch], dtype='float32')

        old_probs, old_v = self.policy_value_net.policy_value(state_batch)
        kl = 0.0
        loss = None
        entropy = None
        for i in range(self.epochs):
            loss, entropy = self.policy_value_net.train_step(state_batch, mcts_probs_batch, winner_batch,
                                                             self.learn_rate * self.lr_multiplier)
            new_probs, new_v = self.policy_value_net.policy_value(state_batch)
            kl = np.mean(np.sum(old_probs * (
                np.log(old_probs + 1e-10) - np.log(new_probs + 1e-10)), axis=1))
            if kl > self.kl_targ * 4:
                break

        if kl > self.kl_targ * 2 and self.lr_multiplier > 0.1:
            self.lr_multiplier /= 1.5
        elif kl < self.kl_targ / 2 and self.lr_multiplier < 10:
            self.lr_multiplier *= 1.5

        # ä¿éšªï¼šé¿å…é™¤ä»¥é›¶
        explained_var_old = 1.0
        explained_var_new = 1.0
        try:
            explained_var_old = 1 - np.var(winner_batch - old_v.flatten()) / np.var(winner_batch)
            explained_var_new = 1 - np.var(winner_batch - new_v.flatten()) / np.var(winner_batch)
        except:
            pass

        print(f"kl:{kl:.5f}, lr_multiplier:{self.lr_multiplier:.3f}, loss:{loss}, entropy:{entropy}, "
              f"explained_var_old:{explained_var_old:.9f}, explained_var_new:{explained_var_new:.9f}")
        return loss, entropy

    def run(self):
        try:
            for i in range(self.game_batch_num):
                # è¼‰å…¥è³‡æ–™ï¼ˆæ ¹æ“šè¨­å®šçš„ TRAIN_DATA_PATHï¼‰
                if not CONFIG['use_redis']:
                    while True:
                        try:
                            with open(TRAIN_DATA_PATH, 'rb') as data_dict:
                                data_file = pickle.load(data_dict)
                                self.data_buffer = data_file['data_buffer']
                                self.iters = data_file['iters']
                            print(f'å·²è¼‰å…¥æœ¬åœ°è³‡æ–™ ({TRAIN_DATA_PATH})')
                            break
                        except Exception as e:
                            print("ç­‰å¾…æœ¬åœ°æ•¸æ“š...", e)
                            time.sleep(30)
                else:
                    while True:
                        try:
                            l = len(self.data_buffer)
                            data = my_redis.get_list_range(self.redis_cli, REDIS_KEY, l if l == 0 else l - 1, -1)
                            self.data_buffer.extend(data)
                            self.iters = self.redis_cli.get(REDIS_ITERS_KEY)
                            if self.redis_cli.llen(REDIS_KEY) > self.buffer_size:
                                self.redis_cli.lpop(REDIS_KEY, self.buffer_size // 10)
                            print(f"å·²è¼‰å…¥ Redis æ•¸æ“š ({REDIS_KEY})")
                            break
                        except Exception as e:
                            print("ç­‰å¾… Redis æ•¸æ“š...", e)
                            time.sleep(5)

                print(f"è¨“ç·´æ‰¹æ¬¡ {i + 1}ï¼Œç´¯ç©æ¨£æœ¬æ•¸ï¼š{len(self.data_buffer)}")
                # ç­‰å¾…è³‡æ–™è¶³å¤ 
                while len(self.data_buffer) <= self.batch_size:
                    print("âš ï¸ è³‡æ–™é‡ä¸è¶³ï¼Œç­‰å¾…æ›´å¤šè³‡æ–™å†è¨“ç·´...")
                    time.sleep(600)

                # åŸ·è¡Œä¸€æ¬¡æ›´æ–°
                if len(self.data_buffer) > self.batch_size:
                    loss, entropy = self.policy_updata()
                    time.sleep(CONFIG['train_update_interval'])

                    # å®šæœŸè©•ä¼°
                    if (i + 1) % self.check_freq == 0:
                        # é‡æ–°è¼‰å…¥ best modelï¼ˆå¦‚æœæª”æ¡ˆå­˜åœ¨ï¼‰ï¼Œé¿å… state mismatch
                        if os.path.exists(BEST_POLICY_PATH):
                            try:
                                self.best_policy_net = PolicyValueNet(model_file=BEST_POLICY_PATH)
                            except Exception as e:
                                print("é‡æ–°è¼‰å…¥ best model å¤±æ•—:", e)
                                self.best_policy_net = None

                        results = self.multi_evaluate()
                        # å– current vs best çš„å‹ç‡ä¾†æ±ºå®šæ˜¯å¦æ›´æ–° best_policy
                        win_ratio = results.get("current_vs_best", results["current_vs_pure"])[0]
                        print(f"ç¬¬ {i + 1} æ‰¹è‡ªå°å¼ˆè¨“ç·´ï¼Œå‹ç‡ï¼š{win_ratio:.3f}")

                        # å„²å­˜æ¢ä»¶ï¼šå‹ç‡è¦è¶…éé–¾å€¼ä¸”æ¯”æ­·å²æœ€ä½³å¥½
                        if win_ratio > 0.55 and win_ratio > self.best_win_ratio:
                            print(f"ğŸ¯ æ–°æœ€ä½³ç­–ç•¥ç™¼ç¾ï¼å‹ç‡ {win_ratio * 100:.2f}% (è¶…éæ­·å²æœ€ä½³ {self.best_win_ratio:.3f})")
                            self.best_win_ratio = win_ratio
                            self.policy_value_net.save_model(CURRENT_POLICY_PATH)
                            self.policy_value_net.save_model(BEST_POLICY_PATH)
                        else:
                            print("å‹ç‡ä¸è¶³ 55% æˆ–æ²’æœ‰è¶…è¶Šæ­·å²æœ€ä½³ï¼Œè·³éä¿å­˜ best_policy")

                        # checkpoint
                        self.policy_value_net.save_model(f'{MODELS_DIR}/current_policy_batch{i + 1}.pth')
        except KeyboardInterrupt:
            print('\nâ›”ï¸ æ‰‹å‹•çµ‚æ­¢è¨“ç·´')


if CONFIG['use_frame'] == 'paddle':
    training_pipeline = TrainPipeline(init_model=CURRENT_POLICY_PATH)
    training_pipeline.run()
elif CONFIG['use_frame'] == 'pytorch':
    training_pipeline = TrainPipeline(init_model=CURRENT_POLICY_PATH)
    training_pipeline.run()
else:
    print('æš‚ä¸æ”¯æŒæ‚¨é€‰æ‹©çš„æ¡†æ¶')
    print('è®­ç»ƒç»“æŸ')